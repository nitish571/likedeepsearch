import os
from dotenv import load_dotenv
from together import Together

# Load environment variables from .env file
load_dotenv()
os.environ["TOGETHER_API_KEY"] = os.getenv("TOGETHER_API_KEY")  # Ensure this is set in the environment

# Initialize the client for the language model

client = Together()

def get_llm_response(message):
    """
    Retrieves a response from a Meta Llama model based on the provided message.

    This function uses the Meta Llama model to generate a response to the input message.
    It sends a request to the model with the message and returns the generated response.

    Parameters:
        message (str): The input message to be sent to the Meta Llama model.

    Returns:
        str: The response generated by the Meta Llama model, or an error message if the request fails.

    Raises:
        Exception: If an error occurs while sending the request or parsing the response.

    Examples:
        >>> get_llm_response("Hello, how are you?")
        'I am doing well, thank you for asking.'
    """
    try:
        response = client.chat.completions.create(
            model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
            messages=[{"role": "user", "content": message}],
            max_tokens=1024,
            temperature=0.7
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error in getting response: {str(e)}"
